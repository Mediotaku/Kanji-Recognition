The particular neurons that are firing when seeing one character class variation are not the same that when seeing another one variation from the same class.


Let's use as an example a seemingly simple task: to recognize a number from a binary image between 0 and 9. Our objective is not only to explain the general working of a multilayer neural network structure, but to actually understand what is behind the process and by what reasons the structure is motivated in the most intuitive way possible.

As we said, our example will recognize handwritten digits. First, we will explain the structure of the model, for afterwards explain the training process and algorithm.

Boom of research torwards these areas:
CNN -> Good for image recognition
LSTM -> Good for speech recognition

Ours is multilayer perceptron, to understand the rest is a prerequisit to understand this first

Here, we can define each "neuron" or node simply as something that holds a number from 0 to 1. For our first layer, each neuron corresponds to the input of an image's pixel. If our image is 28x28 (MNIST standard, explain before), then we have  layer of 784 neurons. Each of them hold a number that represents the greyscale value of its corresponding pixel, from 0 for black pixels, to 1 for white pixels. Each number inside the neuron it's call its "activation". 
Jumping over to our last layer (put an image), this have ten neurons, each representing one of the digits we are willing to classify. The activation in each of these neurons, again ranging form 0 to 1, represents how much the system thinks that a given image correspond with each digit.  

There are also a couples of layers in between called the hidden layers.

The choice of the number of neurons in each of these layer is kind of an arbitrary choice. Even the number of hidden layers is not an evident choice. In practice, these are choosen by experimental tests, adding more when the number of layers seem not being able to capture a dataset's complexity.  

The activations in one layer determine the activations of the next layer. The heart of the network, as an information processing mechanism, comes down to how activations in one layer bring about activations in the next layer.  

In the real brain, some groups of neurons firing, cause others to fire.

Supposing our network is already trained, when feed with a certain digit image, activation in each sucessive layer will provoque the activation in the final output layer, where the biggest activation should be the one of the neuron representing the digit we used as input.

(Maybe this part in the training)
Why we expect our layer to behave like that? What are they suppose to be doing? When our brains recognite digits, it does so by breaking down them in smaller components. For example, an eight is made of two loops, while a nine only have one plus an straight line. In a perfect world, we hope each neuron in the hidden layers correspond with one of this features. This way, a neural networks only have to know with specific combination of features make up each digit. Of course, these features could be broken down into another subfeatures, such as dividing the loop in 4 edges. Again in a perfect model, we could hope each neuron from our first hidden layer correspond to some subfeature, while the second hidden layer's neurons represent the bigger features we described first. In the end, whether or not our network decompose the digits this way it is difficult to know without knowledge of the training process. Breaking down things into layers of abstractions is not only a model for image recognition. For example, in speech recognition certains sound can be taken as features, which eventually will be joined together in syllables adding another layer of abstration to the equation, combining into a output of words or sentences recognition. Extrapolating to this thesis's topic, we could even identify this features as the kanji's radicals we cover in the previous section, if not a subset of them.

Once we understand what we expect our neural network to do, we can deep in the behaviour of the layers themself. As we said, a layer objective could be to know if a certain type of edge is present in one region of the image. The parameters we should twek to make one layer express the presence of a feature have already been mentioned in the our introduction to the perceptron's concept. Giving an input layer with all its neurons connected to one neuron of the next layer, we assign a "weight", possitive o negative, to each of these connections. Then, we multiply each input neuron activation to its connected weight and sum the results. If we wanted to detect whether or not an edge is present in one region, we could assign negative weights to the pixels around that area, so the sum is larger because the edge pixels are brighter than the surrounding pixels. The sum os these multiplications could be any number, but as we want every neuron to hold a value between 0 and 1, an Activation function is required. In short, we are measuring "how possitive" the weighted sum is. Sometimes we don't want a neuron to light up when this sum is only bigger than 0, maybe we need it to only meaningfully start from 5, for example. In that case, a bias value of 5 is added before using the activation function. All in all, this is how our single-layer perceptron model fits inside the multilayer perceptron one.   

(formula apart)

Some activations functions commonly used nowadays are Sigmoid, RElu..etc. Usually activation functions are set to be used in a full layer. Explain in which layer is each one supposed to be.

(images of the three)

A more notetionally correct way of representing the (formula apart) formula is by using matrices. We can take all the input layer activations into one single column and all the weights as a matrix, wh each row corresponding to the connections between one layer aqnd a particular neuron of the next layer. This way, the weighted sums of the activations in one layer can be calculated using the vector product of the matrix and the column. Also, we can organize the biases in another column, so we can sum them directly to the result of the previous vector product. We can then proceed to apply our activation function to each component of the resulting vector.
This type of notation is very relevant,as nowdays GPUs optimize matrix calculations for us. (cite videos)

(formula)



Until know we have only talked about one neuron. In the real model, every neuron from the input layer is connected to every neuron of the next layer, each connection with its own weight and bias. This add up to a respetable 784x16 weights, not taking into account the succesive layers of the network. In total, we are talking about 784x16+16x16+16x10= 13002 weights 16+16+10 biases. We can already deduce why machine learning is always considered a computing intensive task. 

Previously, we have mentioned that the weights of the network should be intitalizated by the start of the process. In fact, when we talk about training our model, we are referrening to finding the right weight and biases. 

---------------------------------------------- Gradient descent

The first concept to come to mind when thinking about training a neural network is the method to follow. What we want in our case is, esencially, an algorithm where you could show a certain amount of training data, which comes in the form of images of handwritten digits along with labes for what are supposed to be, and it will adjust those 8040 weights and 30 biases in order to improve its performance on the training data. Hopefully, this layer structure will mean that what it learns also generalizes to images beyond that training data. In this section we will explain the process to come out with this algorithm and its functioning. To start things off, we are going to initialize all the weights and biases randomly. Needless to say, in this state the network is going to perform worse than expected, with a kind of messy output. To correct this, we need to define a cost function between the expected network's output and the real one. In other words, what we do is add up the squares of the differences between those two outputs. This is the cost of a single training example. This sum is small when the network confidently classifies the image correctly, but is large when the network does not know what it is doing, like in our case. If we consider the average cost over all the training data, we get a measure of how lousy our network is. Therefore, what we want to do to improve the performance of our network is to reduce the value of our cost function. Sometimes, we can find the input that minimize the value of a function explicitly, but this is not feasible for a function with thousands of inputs like ours. What we do in this case is to choose any input in the function and decide in which direction to step to make the output lower. In a 2d function this could be done easily following the slope reference, but in a function with multiple inputs like ours we have to find whatever direction that decreses the cost most quickly. In multi-variable calculus, the ``gradient'' of a function gives you the direction of steepest ascend, i.e., the one that increses the function most quickly. On the contrary, if we take the negative of that gradient, it gives us the direction to step that decreses the function most quickly. This is called \textbf{gradient descent}. Eventually, we will reach a minimum. The problem is that it is not guaranteed to be the global minimum, as it may be simply a local minimum. Finding the global minimum is much more difficult than finding a local one. In other words, we could have found a valley in our function, but not the deepest valley.  This process of taking small steps until reaching a minimum is also the reason why neurons have continuously ranging activations rather than simply on and off states, so these steps are much more smooth.  

(Show a graph with a gradiente descend graph with a local minimum, in other words, a valley but not the deepest valley.)

Notation wise, we can organize all the weights and biases of our network into a large column vector. The negative gradient of the cost function is just another column vector. Adding or substracting the second one values to its corresponding inputs of the first vector will minimize the average cost and therefore improve our network's output for all the samples. The algorithm for computing this gradiente efficiently, which is the core of how neural networks learn, is called \textbf{backpropagation}.    


(explain how the real patterns taken by the layers are, after finding a local minimum)
(explain neural network hyperparameters)

------------------------------------------------- backpropagation

Because of thinking about the gradient vector as a direction in 8040 dimensions is beyond the scope of our imagination, we can think about it in another way. The magnitute of each component of the negative gradient vector is telling us how sensitive the cost function is to each weight and bias. For example, if the negative gradient for an specific weight is 3.20 and for other is 0.10, it means that the cost of the function is x32 times more sensitive to changes in the first weight than in the second.

Because the cost function involves averaging a certain cost per example over all the ten of thousand of training examples, the way that we adjust the weight and biases for a single gradient descent step also depend on every single example \cite{MLVIDEOS}. For the sake of simplification we will explain the backpropagation algorithm through a single example. Starting with an untrained network, the results in the output may look almost random. We can not change this directly, as we only have influence over the weights and biases. However, we can at least hope how the changes in the output activations should be. In short, we want the activation representing our image's digit to go up and the rest to go down. The change should be proporcional to how far away is the activation to its target value. Let us focus in the neuron representing our image's digit, and therefore the one we wish to increase. Looking at the connections to this neuron coming from the previous layer, we would want to increase the weights of the connections with higher activation neurons, as they are the ones which could have a stronger influence over our target. The next thing to look at could be the previous layer activations. If everything connected to our target neuron with a possitive weight increase its activation, and decrese when the weight is negative, then the target neuron will become more active. Of course we can not change directly the activation values, but it is worth to keep track of them. The desired changes determined by the target neuron are added together with the desired ones determined by the rest of neurons of the output layer, again, in proportion to the correspoding weights and in proportion to how much each of those neurons need to change. This way, we get a list of adjustments that we want to happen to the layer. Once we have this, we can recursively apply the same process to the relevant weights that determine those values, repeating the algorithm and moving backwards through the network. This is what we know as \textbf{backpropagation}. As this is only for one sample, we have to repeate the same backprop routine for every training sample, and then average together all the changes. This average is in fact, the negative gradient of the cost function.

In practice, computers do not calculate the influence of every training example over the network's weight and biases at once, as it would take too much time. Instead, usually the training data is shuffled and divide in ``mini-batches'' of a certain number of samples. Then, we compute the gradient descent step for each of these subdivisions. Each ``mini-batch'' gradient descent is only an aproximation of the full training dataset, but it also gives a significant computational speed up. At the end, you will still find a local minimum, only that following a less direct path. 

(hyperparameters and epoch)                
